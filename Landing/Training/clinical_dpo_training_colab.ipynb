{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clinical DPO Training - Google Colab\n",
    "\n",
    "This notebook trains a Llama 3 8B model using Direct Preference Optimization (DPO) on clinical mental health data.\n",
    "\n",
    "**Hardware**: Use GPU runtime (T4 or better)\n",
    "**Time**: ~30-60 minutes\n",
    "**Output**: Trained model ready for download"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Check GPU and Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "!nvidia-smi\n",
    "\n",
    "import torch\n",
    "print(f\"\\nPyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q transformers trl peft accelerate datasets bitsandbytes\n",
    "\n",
    "print(\"‚úÖ Dependencies installed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Upload Your Training Data\n",
    "\n",
    "**Important**: Upload these files from your Mac:\n",
    "- `Landing/DPO/Data/dpo_train_dataset.jsonl` (526 pairs)\n",
    "- `Landing/DPO/Data/dpo_holdout_dataset.jsonl` (59 pairs)\n",
    "\n",
    "Click the folder icon on the left ‚Üí Upload ‚Üí Select both files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify uploaded files\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "required_files = ['dpo_train_dataset.jsonl', 'dpo_holdout_dataset.jsonl']\n",
    "missing_files = [f for f in required_files if not Path(f).exists()]\n",
    "\n",
    "if missing_files:\n",
    "    print(\"‚ùå Missing files:\")\n",
    "    for f in missing_files:\n",
    "        print(f\"   - {f}\")\n",
    "    print(\"\\nüì§ Please upload the files using the file browser on the left\")\n",
    "else:\n",
    "    print(\"‚úÖ All training files found!\")\n",
    "    !wc -l *.jsonl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from trl import DPOTrainer, DPOConfig\n",
    "from datasets import load_dataset\n",
    "from datetime import datetime\n",
    "\n",
    "# Configuration\n",
    "BASE_MODEL = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "OUTPUT_DIR = \"./clinical_dpo_model\"\n",
    "TRAIN_DATA = \"dpo_train_dataset.jsonl\"\n",
    "\n",
    "# Training hyperparameters\n",
    "BATCH_SIZE = 4              # T4 GPU can handle larger batches\n",
    "GRAD_ACCUM_STEPS = 2        # Effective batch size = 8\n",
    "NUM_EPOCHS = 3\n",
    "LEARNING_RATE = 5e-6\n",
    "MAX_SEQ_LENGTH = 2048       # Full length for long responses\n",
    "BETA = 0.1\n",
    "\n",
    "# LoRA configuration\n",
    "LORA_R = 16\n",
    "LORA_ALPHA = 16\n",
    "LORA_DROPOUT = 0.05\n",
    "TARGET_MODULES = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]\n",
    "\n",
    "print(\"‚úÖ Configuration loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"üìÇ Loading training data...\")\n",
    "dataset = load_dataset(\"json\", data_files=TRAIN_DATA, split=\"train\")\n",
    "\n",
    "print(f\"‚úÖ Loaded {len(dataset)} training examples\")\n",
    "print(f\"\\nüìù Sample entry:\")\n",
    "sample = dataset[0]\n",
    "print(f\"   Prompt: {sample['prompt'][:100]}...\")\n",
    "print(f\"   Chosen: {sample['chosen'][:100]}...\")\n",
    "print(f\"   Rejected: {sample['rejected'][:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Load Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"üîÑ Loading {BASE_MODEL}...\")\n",
    "print(\"   This may take a few minutes (downloading ~16GB)...\\n\")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "# Load model with 4-bit quantization (saves memory)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    load_in_4bit=True,  # Use 4-bit quantization\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Model loaded successfully\")\n",
    "print(f\"   Device: {next(model.parameters()).device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Add LoRA Adapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=LORA_R,\n",
    "    lora_alpha=LORA_ALPHA,\n",
    "    target_modules=TARGET_MODULES,\n",
    "    lora_dropout=LORA_DROPOUT,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Print trainable parameters\n",
    "trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "total = sum(p.numel() for p in model.parameters())\n",
    "\n",
    "print(f\"‚úÖ LoRA adapters configured:\")\n",
    "print(f\"   Trainable params: {trainable:,} ({trainable/total*100:.2f}%)\")\n",
    "print(f\"   Total params: {total:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Configure DPO Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = DPOConfig(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    gradient_accumulation_steps=GRAD_ACCUM_STEPS,\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    max_length=MAX_SEQ_LENGTH,\n",
    "    max_prompt_length=512,\n",
    "    beta=BETA,\n",
    "    \n",
    "    # GPU optimization\n",
    "    bf16=True,  # Use bfloat16 on GPU\n",
    "    fp16=False,\n",
    "    \n",
    "    # Logging and saving\n",
    "    logging_steps=10,\n",
    "    save_steps=50,\n",
    "    save_total_limit=2,\n",
    "    \n",
    "    # Optimization\n",
    "    optim=\"adamw_torch\",\n",
    "    warmup_ratio=0.1,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    \n",
    "    # Other\n",
    "    remove_unused_columns=False,\n",
    "    report_to=\"none\",\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Training configuration ready\")\n",
    "print(f\"   Effective batch size: {BATCH_SIZE * GRAD_ACCUM_STEPS}\")\n",
    "print(f\"   Total steps: ~{len(dataset) // (BATCH_SIZE * GRAD_ACCUM_STEPS) * NUM_EPOCHS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Start Training üöÄ\n",
    "\n",
    "**Expected time**: 30-60 minutes on T4 GPU\n",
    "\n",
    "You'll see:\n",
    "- Loss values every 10 steps\n",
    "- Progress bar showing estimated completion time\n",
    "- Checkpoints saved every 50 steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"STARTING DPO TRAINING\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "start_time = datetime.now()\n",
    "print(f\"\\nüöÄ Started at: {start_time.strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "\n",
    "try:\n",
    "    trainer = DPOTrainer(\n",
    "        model=model,\n",
    "        ref_model=None,\n",
    "        args=training_args,\n",
    "        train_dataset=dataset,\n",
    "        processing_class=tokenizer,\n",
    "    )\n",
    "    \n",
    "    trainer.train()\n",
    "    \n",
    "    end_time = datetime.now()\n",
    "    duration = end_time - start_time\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"‚úÖ TRAINING COMPLETED!\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"   Duration: {duration}\")\n",
    "    print(f\"   Model saved to: {OUTPUT_DIR}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Training failed: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Save Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_output = f\"{OUTPUT_DIR}/final_model\"\n",
    "\n",
    "print(f\"üíæ Saving final model to {final_output}...\")\n",
    "trainer.model.save_pretrained(final_output)\n",
    "tokenizer.save_pretrained(final_output)\n",
    "\n",
    "print(f\"\\n‚úÖ Model saved!\")\n",
    "print(f\"\\nüìÅ Files created:\")\n",
    "!ls -lh {final_output}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Download Trained Model\n",
    "\n",
    "Zip the model for easy download:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "print(\"üì¶ Creating zip file...\")\n",
    "shutil.make_archive('clinical_dpo_model', 'zip', OUTPUT_DIR, 'final_model')\n",
    "\n",
    "print(\"\\n‚úÖ Model packaged!\")\n",
    "print(\"\\nüì• Download the file:\")\n",
    "print(\"   1. Click the folder icon on the left\")\n",
    "print(\"   2. Find 'clinical_dpo_model.zip'\")\n",
    "print(\"   3. Right-click ‚Üí Download\")\n",
    "print(\"\\n   Or run this to download:\")\n",
    "\n",
    "from google.colab import files\n",
    "files.download('clinical_dpo_model.zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 11: Quick Inference Test (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the trained model\n",
    "test_prompt = \"I've been feeling really anxious about my upcoming presentation at work.\"\n",
    "\n",
    "formatted_prompt = f\"<|user|>\\n{test_prompt}\\n<|assistant|>\\n\"\n",
    "inputs = tokenizer(formatted_prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "print(f\"üß™ Testing trained model...\\n\")\n",
    "print(f\"Patient: {test_prompt}\\n\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=200,\n",
    "        temperature=0.7,\n",
    "        do_sample=True\n",
    "    )\n",
    "\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "response = response.split(\"<|assistant|>\")[-1].strip()\n",
    "\n",
    "print(f\"Therapist: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚úÖ Training Complete!\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "1. **Download the model**: The zip file contains your trained LoRA adapters\n",
    "2. **Transfer to Mac**: Unzip in `Landing/Training/outputs/final_model/`\n",
    "3. **Run evaluation**: Use your Mac to run `evaluate_model.py`\n",
    "4. **Test inference**: Run `inference_test.py` with sample prompts\n",
    "\n",
    "### Model Usage on Mac:\n",
    "\n",
    "```python\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"./outputs/final_model\",\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
