{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üè• Clinical LLM Training with DPO + Weaver Evaluation\n",
    "\n",
    "**Goal**: Train Llama 3 8B with Direct Preference Optimization (DPO) on clinical preference pairs, then evaluate using Weaver ensemble scoring.\n",
    "\n",
    "**Dataset**: 526 training pairs + 59 holdout pairs (filtered by Weaver from 2,742 Gemini-generated pairs)\n",
    "\n",
    "**Runtime**: ~20-25 minutes total (15 min training + 5-10 min evaluation)\n",
    "\n",
    "---\n",
    "\n",
    "## üìã Checklist\n",
    "\n",
    "Before running:\n",
    "- [ ] Set Runtime to GPU (Runtime ‚Üí Change runtime type ‚Üí T4 GPU)\n",
    "- [ ] Have your data files ready: `dpo_train_dataset.jsonl`, `dpo_holdout_dataset.jsonl`\n",
    "- [ ] Have Weaver code ready: `weaver_ensembles.py`\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "print(\"üì¶ Installing Unsloth and dependencies...\")\n",
    "!pip install -q unsloth \"xformers<0.0.26\" trl datasets accelerate transformers torch\n",
    "\n",
    "# Additional packages for evaluation\n",
    "!pip install -q sentence-transformers scikit-learn\n",
    "\n",
    "print(\"‚úÖ Installation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Upload Data Files\n",
    "\n",
    "Upload your files using the file browser on the left, or run the cell below to upload via dialog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "import os\n",
    "\n",
    "# Create directories\n",
    "os.makedirs('Data', exist_ok=True)\n",
    "os.makedirs('Weaver', exist_ok=True)\n",
    "\n",
    "print(\"üìÇ Please upload the following files:\")\n",
    "print(\"   1. dpo_train_dataset.jsonl\")\n",
    "print(\"   2. dpo_holdout_dataset.jsonl\")\n",
    "print(\"   3. weaver_ensembles.py\")\n",
    "print(\"   4. weaver_weights.json (optional, if you have trained weights)\")\n",
    "print(\"\\n‚¨ÜÔ∏è  Click 'Choose Files' below...\\n\")\n",
    "\n",
    "uploaded = files.upload()\n",
    "\n",
    "# Move files to appropriate directories\n",
    "for filename in uploaded.keys():\n",
    "    if 'dpo' in filename and filename.endswith('.jsonl'):\n",
    "        !mv \"{filename}\" Data/\n",
    "        print(f\"‚úÖ Moved {filename} to Data/\")\n",
    "    elif 'weaver' in filename:\n",
    "        !mv \"{filename}\" Weaver/\n",
    "        print(f\"‚úÖ Moved {filename} to Weaver/\")\n",
    "\n",
    "print(\"\\nüìã File check:\")\n",
    "!ls -lh Data/\n",
    "!ls -lh Weaver/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Data Validation\n",
    "\n",
    "Let's verify the data format before training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load and inspect training data\n",
    "print(\"üîç Validating training data...\")\n",
    "train_dataset = load_dataset(\"json\", data_files=\"Data/dpo_train_dataset.jsonl\", split=\"train\")\n",
    "holdout_dataset = load_dataset(\"json\", data_files=\"Data/dpo_holdout_dataset.jsonl\", split=\"train\")\n",
    "\n",
    "print(f\"\\n‚úÖ Training samples: {len(train_dataset)}\")\n",
    "print(f\"‚úÖ Holdout samples: {len(holdout_dataset)}\")\n",
    "\n",
    "# Show sample\n",
    "sample = train_dataset[0]\n",
    "print(\"\\nüìã Sample Entry:\")\n",
    "print(f\"   Prompt: {sample['prompt'][:100]}...\")\n",
    "print(f\"   Chosen: {sample['chosen'][:100]}...\")\n",
    "print(f\"   Rejected: {sample['rejected'][:100]}...\")\n",
    "\n",
    "# Check for required fields\n",
    "required_fields = ['prompt', 'chosen', 'rejected']\n",
    "missing_fields = [field for field in required_fields if field not in sample]\n",
    "\n",
    "if missing_fields:\n",
    "    print(f\"\\n‚ùå ERROR: Missing fields: {missing_fields}\")\n",
    "else:\n",
    "    print(\"\\n‚úÖ All required fields present. Ready for training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Train DPO Model\n",
    "\n",
    "This will take ~15-20 minutes on T4 GPU.\n",
    "\n",
    "**What's happening:**\n",
    "- Loading Llama 3 8B Instruct in 4-bit\n",
    "- Adding LoRA adapters (trainable parameters)\n",
    "- Training with DPO for 3 epochs\n",
    "- Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from unsloth import FastLanguageModel, PatchDPOTrainer, is_bfloat16_supported\n",
    "from trl import DPOConfig, DPOTrainer\n",
    "from datasets import load_dataset\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"üöÄ STARTING CLINICAL DPO TRAINING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Configuration\n",
    "MAX_SEQ_LENGTH = 2048\n",
    "NUM_EPOCHS = 3\n",
    "LEARNING_RATE = 5e-6\n",
    "BATCH_SIZE = 2\n",
    "GRAD_ACCUMULATION = 4\n",
    "OUTPUT_DIR = \"clinical_dpo_model_v1\"\n",
    "\n",
    "print(f\"\\n‚öôÔ∏è  Configuration:\")\n",
    "print(f\"   Epochs: {NUM_EPOCHS}\")\n",
    "print(f\"   Learning Rate: {LEARNING_RATE}\")\n",
    "print(f\"   Effective Batch Size: {BATCH_SIZE * GRAD_ACCUMULATION}\")\n",
    "print(f\"   Max Sequence Length: {MAX_SEQ_LENGTH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load base model\n",
    "print(\"\\nü§ñ Loading Llama 3 8B Instruct...\")\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=\"unsloth/llama-3-8b-instruct-bnb-4bit\",\n",
    "    max_seq_length=MAX_SEQ_LENGTH,\n",
    "    dtype=None,\n",
    "    load_in_4bit=True,\n",
    ")\n",
    "print(\"   ‚úÖ Base model loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add LoRA adapters\n",
    "print(\"\\nüîß Adding LoRA adapters...\")\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=16,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                    \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0,\n",
    "    bias=\"none\",\n",
    "    use_gradient_checkpointing=\"unsloth\",\n",
    "    random_state=3407,\n",
    ")\n",
    "print(\"   ‚úÖ LoRA configured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and format dataset\n",
    "print(\"\\nüìÇ Loading training data...\")\n",
    "\n",
    "def format_dpo(example):\n",
    "    prompt = example['prompt']\n",
    "    if not prompt.startswith(\"<|user|>\"):\n",
    "        prompt = f\"<|user|>\\n{prompt}\\n<|assistant|>\\n\"\n",
    "    return {\n",
    "        \"prompt\": prompt,\n",
    "        \"chosen\": example['chosen'],\n",
    "        \"rejected\": example['rejected']\n",
    "    }\n",
    "\n",
    "dataset = load_dataset(\"json\", data_files=\"Data/dpo_train_dataset.jsonl\", split=\"train\")\n",
    "dataset = dataset.map(format_dpo)\n",
    "print(f\"   ‚úÖ Loaded {len(dataset)} training pairs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize trainer\n",
    "print(\"\\n‚öôÔ∏è  Initializing DPO Trainer...\")\n",
    "\n",
    "patch_dpo = PatchDPOTrainer()\n",
    "\n",
    "dpo_trainer = DPOTrainer(\n",
    "    model=model,\n",
    "    ref_model=None,\n",
    "    tokenizer=tokenizer,\n",
    "    beta=0.1,\n",
    "    train_dataset=dataset,\n",
    "    args=DPOConfig(\n",
    "        per_device_train_batch_size=BATCH_SIZE,\n",
    "        gradient_accumulation_steps=GRAD_ACCUMULATION,\n",
    "        warmup_ratio=0.1,\n",
    "        num_train_epochs=NUM_EPOCHS,\n",
    "        learning_rate=LEARNING_RATE,\n",
    "        fp16=not is_bfloat16_supported(),\n",
    "        bf16=is_bfloat16_supported(),\n",
    "        logging_steps=10,\n",
    "        save_steps=100,\n",
    "        output_dir=OUTPUT_DIR,\n",
    "        optim=\"adamw_8bit\",\n",
    "        seed=42,\n",
    "        remove_unused_columns=False,\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(\"   ‚úÖ Trainer ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train!\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üéØ TRAINING STARTED\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Training {len(dataset)} samples for {NUM_EPOCHS} epochs...\")\n",
    "print(f\"Expected steps: ~{len(dataset) * NUM_EPOCHS // (BATCH_SIZE * GRAD_ACCUMULATION)}\")\n",
    "print(\"\\n‚è±Ô∏è  This will take ~15-20 minutes. Watch the loss decrease!\\n\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "trainer_output = dpo_trainer.train()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚úÖ TRAINING COMPLETE!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Final loss: {trainer_output.training_loss:.4f}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "print(f\"\\nüíæ Saving model to {OUTPUT_DIR}...\")\n",
    "model.save_pretrained(OUTPUT_DIR)\n",
    "tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "print(\"   ‚úÖ Model saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ Quick Inference Test\n",
    "\n",
    "Let's test if the model works before full evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üß™ Running quick inference test...\\n\")\n",
    "\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "test_prompt = \"I've been feeling really anxious about my upcoming presentation at work.\"\n",
    "formatted_prompt = f\"<|user|>\\n{test_prompt}\\n<|assistant|>\\n\"\n",
    "\n",
    "inputs = tokenizer(formatted_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=256,\n",
    "    temperature=0.7,\n",
    "    do_sample=True,\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "response = response.split(\"<|assistant|>\")[-1].strip()\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"üë§ PATIENT:\")\n",
    "print(test_prompt)\n",
    "print(\"\\nü§ñ FINE-TUNED MODEL:\")\n",
    "print(response)\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6Ô∏è‚É£ Weaver Evaluation\n",
    "\n",
    "Now the important part: evaluate using Weaver's 5-verifier ensemble!\n",
    "\n",
    "**What we'll do:**\n",
    "1. Load base Llama 3 + your fine-tuned model\n",
    "2. Generate responses from both for 59 holdout prompts\n",
    "3. Score each with Weaver (Clinical Correctness, Therapeutic Tone, Safety, Protocol, Logic)\n",
    "4. Calculate win rate and improvement\n",
    "\n",
    "**Success criteria:**\n",
    "- Win rate > 70%\n",
    "- Average improvement > 0.10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Weaver\n",
    "import sys\n",
    "sys.path.append('Weaver')\n",
    "\n",
    "print(\"üìä Importing Weaver ensemble...\")\n",
    "\n",
    "try:\n",
    "    from weaver_ensembles import (\n",
    "        WeaverEnsemble,\n",
    "        ClinicalCorrectnessVerifier,\n",
    "        TherapeuticToneVerifier,\n",
    "        SafetyVerifier,\n",
    "        ClinicalProtocolVerifier,\n",
    "        DialogueLogicVerifier\n",
    "    )\n",
    "    print(\"   ‚úÖ Weaver imported successfully\")\n",
    "except ImportError as e:\n",
    "    print(f\"   ‚ùå Error importing Weaver: {e}\")\n",
    "    print(\"   Make sure you uploaded weaver_ensembles.py to the Weaver/ folder\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Weaver verifiers\n",
    "import os\n",
    "\n",
    "print(\"üèóÔ∏è  Initializing Weaver jury (5 verifiers)...\\n\")\n",
    "\n",
    "verifiers = [\n",
    "    ClinicalCorrectnessVerifier(device=0),\n",
    "    TherapeuticToneVerifier(device=0),\n",
    "    SafetyVerifier(device=0),\n",
    "    ClinicalProtocolVerifier(),\n",
    "    DialogueLogicVerifier(device=0)\n",
    "]\n",
    "\n",
    "# Load trained weights if available\n",
    "if os.path.exists('Weaver/weaver_weights.json'):\n",
    "    print(\"   Loading trained weights...\")\n",
    "    with open('Weaver/weaver_weights.json', 'r') as f:\n",
    "        weights = json.load(f)\n",
    "        for v in verifiers:\n",
    "            if v.name in weights:\n",
    "                v.weight = weights[v.name]\n",
    "                print(f\"   ‚Ä¢ {v.name}: {v.weight:.2f}\")\n",
    "else:\n",
    "    print(\"   Using default weights:\")\n",
    "    for v in verifiers:\n",
    "        print(f\"   ‚Ä¢ {v.name}: {v.weight:.2f}\")\n",
    "\n",
    "ensemble = WeaverEnsemble(verifiers)\n",
    "print(\"\\n   ‚úÖ Weaver ensemble ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load both models for comparison\n",
    "print(\"\\nü§ñ Loading models for comparison...\\n\")\n",
    "\n",
    "# Base model\n",
    "print(\"   Loading base Llama 3 8B...\")\n",
    "base_model, base_tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=\"unsloth/llama-3-8b-instruct-bnb-4bit\",\n",
    "    max_seq_length=2048,\n",
    "    load_in_4bit=True,\n",
    ")\n",
    "FastLanguageModel.for_inference(base_model)\n",
    "print(\"   ‚úÖ Base model loaded\")\n",
    "\n",
    "# Fine-tuned model\n",
    "print(\"   Loading fine-tuned model...\")\n",
    "finetuned_model, finetuned_tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=OUTPUT_DIR,\n",
    "    max_seq_length=2048,\n",
    "    load_in_4bit=True,\n",
    ")\n",
    "FastLanguageModel.for_inference(finetuned_model)\n",
    "print(\"   ‚úÖ Fine-tuned model loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load holdout dataset\n",
    "print(\"\\nüìã Loading holdout dataset...\")\n",
    "holdout = load_dataset(\"json\", data_files=\"Data/dpo_holdout_dataset.jsonl\", split=\"train\")\n",
    "print(f\"   ‚úÖ Loaded {len(holdout)} holdout samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate responses and score with Weaver\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "print(\"\\nüî¨ Generating responses and scoring with Weaver...\")\n",
    "print(f\"   This will take ~{len(holdout) * 10} seconds (2 models √ó {len(holdout)} samples)\\n\")\n",
    "\n",
    "results = []\n",
    "MAX_NEW_TOKENS = 256\n",
    "TEMPERATURE = 0.7\n",
    "\n",
    "for idx, example in enumerate(tqdm(holdout, desc=\"Evaluating\")):\n",
    "    prompt = example['prompt']\n",
    "    chosen_gold = example['chosen']\n",
    "    \n",
    "    # Extract clean user text\n",
    "    user_text = prompt.replace(\"<|user|>\", \"\").replace(\"<|assistant|>\", \"\").strip()\n",
    "    if \"\\n\" in user_text:\n",
    "        user_text = user_text.split(\"\\n\")[0].strip()\n",
    "    \n",
    "    # Generate from base model\n",
    "    base_inputs = base_tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "    with torch.no_grad():\n",
    "        base_outputs = base_model.generate(\n",
    "            **base_inputs,\n",
    "            max_new_tokens=MAX_NEW_TOKENS,\n",
    "            temperature=TEMPERATURE,\n",
    "            do_sample=True,\n",
    "            pad_token_id=base_tokenizer.eos_token_id\n",
    "        )\n",
    "    base_response = base_tokenizer.decode(base_outputs[0], skip_special_tokens=True)\n",
    "    base_response = base_response.split(\"<|assistant|>\")[-1].strip()\n",
    "    \n",
    "    # Generate from fine-tuned model\n",
    "    ft_inputs = finetuned_tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "    with torch.no_grad():\n",
    "        ft_outputs = finetuned_model.generate(\n",
    "            **ft_inputs,\n",
    "            max_new_tokens=MAX_NEW_TOKENS,\n",
    "            temperature=TEMPERATURE,\n",
    "            do_sample=True,\n",
    "            pad_token_id=finetuned_tokenizer.eos_token_id\n",
    "        )\n",
    "    ft_response = finetuned_tokenizer.decode(ft_outputs[0], skip_special_tokens=True)\n",
    "    ft_response = ft_response.split(\"<|assistant|>\")[-1].strip()\n",
    "    \n",
    "    # Score with Weaver\n",
    "    base_eval = ensemble.evaluate_pair(chosen_gold, base_response, user_text)\n",
    "    ft_eval = ensemble.evaluate_pair(chosen_gold, ft_response, user_text)\n",
    "    \n",
    "    improvement = ft_eval['rejected_score'] - base_eval['rejected_score']\n",
    "    \n",
    "    results.append({\n",
    "        \"sample_id\": idx,\n",
    "        \"prompt\": user_text[:200],\n",
    "        \"base_response\": base_response,\n",
    "        \"finetuned_response\": ft_response,\n",
    "        \"base_score\": float(base_eval['rejected_score']),\n",
    "        \"finetuned_score\": float(ft_eval['rejected_score']),\n",
    "        \"improvement\": float(improvement),\n",
    "        \"win\": improvement > 0\n",
    "    })\n",
    "\n",
    "print(\"\\n   ‚úÖ Evaluation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7Ô∏è‚É£ Results Analysis\n",
    "\n",
    "Let's see how the fine-tuned model performed!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate metrics\n",
    "wins = sum(1 for r in results if r['win'])\n",
    "losses = len(results) - wins\n",
    "win_rate = wins / len(results) * 100\n",
    "\n",
    "avg_base = np.mean([r['base_score'] for r in results])\n",
    "avg_ft = np.mean([r['finetuned_score'] for r in results])\n",
    "avg_improvement = avg_ft - avg_base\n",
    "\n",
    "std_base = np.std([r['base_score'] for r in results])\n",
    "std_ft = np.std([r['finetuned_score'] for r in results])\n",
    "\n",
    "# Print results\n",
    "print(\"=\"*60)\n",
    "print(\"üéØ EVALUATION RESULTS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nüìà Overall Performance:\")\n",
    "print(f\"   Win Rate (Fine-tuned > Base):  {win_rate:.1f}% ({wins}/{len(results)})\")\n",
    "print(f\"   Loss Rate (Fine-tuned ‚â§ Base): {100-win_rate:.1f}% ({losses}/{len(results)})\")\n",
    "print(f\"\\nüìä Weaver Scores:\")\n",
    "print(f\"   Base Model Average:            {avg_base:.3f} (¬±{std_base:.3f})\")\n",
    "print(f\"   Fine-tuned Model Average:      {avg_ft:.3f} (¬±{std_ft:.3f})\")\n",
    "print(f\"   Average Improvement:           {'+' if avg_improvement > 0 else ''}{avg_improvement:.3f}\")\n",
    "\n",
    "# Success criteria\n",
    "print(\"\\nüéØ Success Criteria:\")\n",
    "success = []\n",
    "\n",
    "if win_rate > 70:\n",
    "    print(\"   ‚úÖ Win Rate > 70%\")\n",
    "    success.append(True)\n",
    "else:\n",
    "    print(f\"   ‚ùå Win Rate ‚â§ 70% (got {win_rate:.1f}%)\")\n",
    "    success.append(False)\n",
    "\n",
    "if avg_improvement > 0.10:\n",
    "    print(\"   ‚úÖ Avg Improvement > 0.10\")\n",
    "    success.append(True)\n",
    "else:\n",
    "    print(f\"   ‚ùå Avg Improvement ‚â§ 0.10 (got {avg_improvement:.3f})\")\n",
    "    success.append(False)\n",
    "\n",
    "print(\"=\"*60)\n",
    "\n",
    "if all(success):\n",
    "    print(\"üéâ SUCCESS: Model shows significant clinical improvement!\")\n",
    "elif win_rate > 60:\n",
    "    print(\"‚ö†Ô∏è  PARTIAL SUCCESS: Model improved but below target\")\n",
    "else:\n",
    "    print(\"‚ùå NEEDS IMPROVEMENT: Model did not show consistent improvement\")\n",
    "\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize score distributions\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Score comparison\n",
    "ax1.hist([r['base_score'] for r in results], bins=20, alpha=0.5, label='Base Model', color='red')\n",
    "ax1.hist([r['finetuned_score'] for r in results], bins=20, alpha=0.5, label='Fine-tuned', color='green')\n",
    "ax1.axvline(avg_base, color='red', linestyle='--', linewidth=2, label=f'Base Avg: {avg_base:.3f}')\n",
    "ax1.axvline(avg_ft, color='green', linestyle='--', linewidth=2, label=f'Fine-tuned Avg: {avg_ft:.3f}')\n",
    "ax1.set_xlabel('Weaver Score')\n",
    "ax1.set_ylabel('Frequency')\n",
    "ax1.set_title('Score Distribution Comparison')\n",
    "ax1.legend()\n",
    "ax1.grid(alpha=0.3)\n",
    "\n",
    "# Improvement distribution\n",
    "improvements = [r['improvement'] for r in results]\n",
    "ax2.hist(improvements, bins=20, color='blue', alpha=0.7, edgecolor='black')\n",
    "ax2.axvline(0, color='red', linestyle='--', linewidth=2, label='No Change')\n",
    "ax2.axvline(avg_improvement, color='green', linestyle='--', linewidth=2, label=f'Avg: {avg_improvement:.3f}')\n",
    "ax2.set_xlabel('Improvement (Fine-tuned - Base)')\n",
    "ax2.set_ylabel('Frequency')\n",
    "ax2.set_title('Improvement Distribution')\n",
    "ax2.legend()\n",
    "ax2.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('evaluation_charts.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"üìä Charts saved to evaluation_charts.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show top improvements\n",
    "sorted_results = sorted(results, key=lambda x: x['improvement'], reverse=True)\n",
    "top_5 = sorted_results[:5]\n",
    "\n",
    "print(\"\\nüìù Top 5 Improvements:\\n\")\n",
    "for i, ex in enumerate(top_5, 1):\n",
    "    print(\"‚îÄ\"*60)\n",
    "    print(f\"EXAMPLE {i} | Improvement: +{ex['improvement']:.3f}\")\n",
    "    print(\"‚îÄ\"*60)\n",
    "    print(f\"üë§ PATIENT:\\n{ex['prompt']}\\n\")\n",
    "    print(f\"ü§ñ BASE (score={ex['base_score']:.3f}):\\n{ex['base_response'][:200]}...\\n\")\n",
    "    print(f\"‚ú® FINE-TUNED (score={ex['finetuned_score']:.3f}):\\n{ex['finetuned_response'][:200]}...\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save detailed results\n",
    "output_data = {\n",
    "    \"summary\": {\n",
    "        \"total_samples\": len(results),\n",
    "        \"win_rate\": float(win_rate),\n",
    "        \"wins\": wins,\n",
    "        \"losses\": losses,\n",
    "        \"avg_base_score\": float(avg_base),\n",
    "        \"avg_finetuned_score\": float(avg_ft),\n",
    "        \"avg_improvement\": float(avg_improvement),\n",
    "        \"success_criteria_met\": all(success)\n",
    "    },\n",
    "    \"detailed_results\": results\n",
    "}\n",
    "\n",
    "with open(\"evaluation_results.json\", \"w\") as f:\n",
    "    json.dump(output_data, f, indent=2)\n",
    "\n",
    "print(\"üíæ Results saved to evaluation_results.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8Ô∏è‚É£ Download Results\n",
    "\n",
    "Download your trained model and evaluation results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "import shutil\n",
    "\n",
    "print(\"üì¶ Preparing files for download...\\n\")\n",
    "\n",
    "# Zip the model\n",
    "print(\"   Zipping model (this may take a minute)...\")\n",
    "!zip -r clinical_dpo_model_v1.zip clinical_dpo_model_v1/\n",
    "print(\"   ‚úÖ Model zipped\")\n",
    "\n",
    "# Download files\n",
    "print(\"\\nüì• Downloading files...\\n\")\n",
    "print(\"   1. Trained model (large file, ~500MB-1GB)\")\n",
    "files.download('clinical_dpo_model_v1.zip')\n",
    "\n",
    "print(\"   2. Evaluation results (JSON)\")\n",
    "files.download('evaluation_results.json')\n",
    "\n",
    "print(\"   3. Evaluation charts (PNG)\")\n",
    "files.download('evaluation_charts.png')\n",
    "\n",
    "print(\"\\n‚úÖ All files downloaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9Ô∏è‚É£ Next Steps\n",
    "\n",
    "Based on your results:\n",
    "\n",
    "### If Success (Win Rate > 70%):\n",
    "1. **Scale up data generation** to 5,000-10,000 pairs\n",
    "2. **Run trajectory analysis** (multi-turn conversation test)\n",
    "3. **Consider GRPO** for further improvement\n",
    "4. **Integrate Hourglass Emotions** into Weaver\n",
    "\n",
    "### If Partial Success (60-70% Win Rate):\n",
    "1. **Analyze failure cases** above to identify patterns\n",
    "2. **Improve data quality** by adjusting Weaver filters\n",
    "3. **Try different hyperparameters**:\n",
    "   - Lower learning rate (1e-6)\n",
    "   - More epochs (5-7)\n",
    "   - Higher LoRA rank (32)\n",
    "\n",
    "### If Needs Improvement (<60% Win Rate):\n",
    "1. **Check training loss** - did it converge?\n",
    "2. **Validate data quality** - are chosen/rejected truly different?\n",
    "3. **Review Weaver weights** - are they appropriate?\n",
    "4. **Consider different base model** (Qwen 2.5, Mistral, etc.)\n",
    "\n",
    "---\n",
    "\n",
    "**Good luck! üöÄ**"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
